{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.4 MB 69 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.18.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/connor/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /home/connor/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/connor/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/connor/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/connor/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=babd2f8b7c1f6f5630091dbcdc311f29f505f51bec21d1d35da4930c469430f4\n",
      "  Stored in directory: /home/connor/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=903fdbceb24d23551d6a1889441736eb041bc1ae0bbe05be8053a4cfbea99a42\n",
      "  Stored in directory: /home/connor/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.14.1 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.3.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-2.2.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: wcwidth in /home/connor/anaconda3/lib/python3.8/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BicepCurls' 'Posture' 'SideLunges' 'Sitting' 'Squats' 'Standing']\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Path to data directory\n",
    "path = r'/home/connor/Desktop/Pastebles/data'\n",
    "\n",
    "# Holder list\n",
    "li = []\n",
    "\n",
    "# Regex changes the target class creation\n",
    "# 'CORRECT|WRONG|[a-zA-Z]+(?=_[0-9]+_data)|(?<=IR_[0-9]_)[a-zA-Z]+'\n",
    "\n",
    "# Will grab any subfolders from path and their csv files\n",
    "\n",
    "for filename in Path(path).rglob('*.csv'):\n",
    "    # Reads individual csv files\n",
    "    # print(filename)\n",
    "    # print(\"\\n\")\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    # Matches specifics from the filename using regex (subject to change depending on filenaming convention)   \n",
    "    match  = re.findall('[a-zA-Z]+(?=_[0-9]+_data)', str(filename))\n",
    "    y = (''.join(match))\n",
    "    # Adds target column for classification\n",
    "    df['y'] = y\n",
    "    df['filename'] = filename\n",
    "    # Appends the dataframe to the list\n",
    "    li.append(df)\n",
    "\n",
    "# Concats all data into one dataframe for training/testing\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Target column is y\n",
    "y_string = frame['y']\n",
    "actions = ['BicepCurls', 'Posture', 'SideLunges', 'Sitting', 'Squats', 'Standing']\n",
    "\n",
    "# Changes target from string to numeric\n",
    "le = LabelEncoder().fit(actions)\n",
    "y = le.transform(y_string.ravel())\n",
    "\n",
    "print(le.classes_)\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "# Sets the X data\n",
    "filelist = frame['filename']\n",
    "X = frame.drop(['y','arrival_time', 'filename'],axis=1).to_numpy()\n",
    "\n",
    "# Splits data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 10, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['ir_dist'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test)[6].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes:\n",
      "\n",
      "BicepCurls\n",
      "Posture\n",
      "SideLunges\n",
      "Sitting\n",
      "Squats\n",
      "Standing\n"
     ]
    }
   ],
   "source": [
    "all_classes = le.classes_\n",
    "print(\"All Classes:\\n\", *all_classes, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3138/3138 [==============================] - 1s 396us/step - loss: 1.4292 - accuracy: 0.4605\n",
      "Epoch 2/50\n",
      "3138/3138 [==============================] - 1s 398us/step - loss: 1.1276 - accuracy: 0.5625\n",
      "Epoch 3/50\n",
      "3138/3138 [==============================] - 1s 393us/step - loss: 1.0334 - accuracy: 0.5995\n",
      "Epoch 4/50\n",
      "3138/3138 [==============================] - 1s 388us/step - loss: 0.9610 - accuracy: 0.6206\n",
      "Epoch 5/50\n",
      "3138/3138 [==============================] - 1s 405us/step - loss: 0.9217 - accuracy: 0.6313\n",
      "Epoch 6/50\n",
      "3138/3138 [==============================] - 1s 436us/step - loss: 0.8879 - accuracy: 0.64500s - los\n",
      "Epoch 7/50\n",
      "3138/3138 [==============================] - 1s 465us/step - loss: 0.8629 - accuracy: 0.6534 - ETA: 0s - loss: 0.8630 - accuracy: 0.\n",
      "Epoch 8/50\n",
      "3138/3138 [==============================] - 1s 432us/step - loss: 0.8398 - accuracy: 0.6600\n",
      "Epoch 9/50\n",
      "3138/3138 [==============================] - 1s 428us/step - loss: 0.8212 - accuracy: 0.66340s - loss: 0.8228 - accuracy: 0.\n",
      "Epoch 10/50\n",
      "3138/3138 [==============================] - 1s 437us/step - loss: 0.8089 - accuracy: 0.6671\n",
      "Epoch 11/50\n",
      "3138/3138 [==============================] - 1s 472us/step - loss: 0.7975 - accuracy: 0.6730\n",
      "Epoch 12/50\n",
      "3138/3138 [==============================] - 1s 428us/step - loss: 0.7814 - accuracy: 0.6789\n",
      "Epoch 13/50\n",
      "3138/3138 [==============================] - 1s 445us/step - loss: 0.7697 - accuracy: 0.6833\n",
      "Epoch 14/50\n",
      "3138/3138 [==============================] - 1s 455us/step - loss: 0.7579 - accuracy: 0.6879\n",
      "Epoch 15/50\n",
      "3138/3138 [==============================] - 1s 463us/step - loss: 0.7503 - accuracy: 0.6918\n",
      "Epoch 16/50\n",
      "3138/3138 [==============================] - 1s 459us/step - loss: 0.7397 - accuracy: 0.6962\n",
      "Epoch 17/50\n",
      "3138/3138 [==============================] - 1s 453us/step - loss: 0.7308 - accuracy: 0.7008\n",
      "Epoch 18/50\n",
      "3138/3138 [==============================] - 1s 460us/step - loss: 0.7256 - accuracy: 0.70220s - loss: 0.7269 - accu\n",
      "Epoch 19/50\n",
      "3138/3138 [==============================] - 1s 464us/step - loss: 0.7219 - accuracy: 0.7060\n",
      "Epoch 20/50\n",
      "3138/3138 [==============================] - 1s 441us/step - loss: 0.7170 - accuracy: 0.7079\n",
      "Epoch 21/50\n",
      "3138/3138 [==============================] - 1s 443us/step - loss: 0.7108 - accuracy: 0.7118\n",
      "Epoch 22/50\n",
      "3138/3138 [==============================] - 1s 445us/step - loss: 0.7075 - accuracy: 0.7132\n",
      "Epoch 23/50\n",
      "3138/3138 [==============================] - 1s 455us/step - loss: 0.7005 - accuracy: 0.7168\n",
      "Epoch 24/50\n",
      "3138/3138 [==============================] - 1s 444us/step - loss: 0.6974 - accuracy: 0.7174\n",
      "Epoch 25/50\n",
      "3138/3138 [==============================] - 1s 453us/step - loss: 0.6948 - accuracy: 0.7205\n",
      "Epoch 26/50\n",
      "3138/3138 [==============================] - 1s 453us/step - loss: 0.6868 - accuracy: 0.7230\n",
      "Epoch 27/50\n",
      "3138/3138 [==============================] - 1s 452us/step - loss: 0.6828 - accuracy: 0.72440s - loss: 0.681\n",
      "Epoch 28/50\n",
      "3138/3138 [==============================] - 1s 439us/step - loss: 0.6796 - accuracy: 0.7273\n",
      "Epoch 29/50\n",
      "3138/3138 [==============================] - 1s 460us/step - loss: 0.6752 - accuracy: 0.72820s\n",
      "Epoch 30/50\n",
      "3138/3138 [==============================] - 1s 456us/step - loss: 0.6734 - accuracy: 0.7311\n",
      "Epoch 31/50\n",
      "3138/3138 [==============================] - 1s 443us/step - loss: 0.6703 - accuracy: 0.7306\n",
      "Epoch 32/50\n",
      "3138/3138 [==============================] - 1s 449us/step - loss: 0.6660 - accuracy: 0.7338\n",
      "Epoch 33/50\n",
      "3138/3138 [==============================] - 1s 454us/step - loss: 0.6646 - accuracy: 0.73430s - loss: 0.666\n",
      "Epoch 34/50\n",
      "3138/3138 [==============================] - 1s 470us/step - loss: 0.6603 - accuracy: 0.7353\n",
      "Epoch 35/50\n",
      "3138/3138 [==============================] - 2s 478us/step - loss: 0.6588 - accuracy: 0.7369\n",
      "Epoch 36/50\n",
      "3138/3138 [==============================] - 1s 469us/step - loss: 0.6554 - accuracy: 0.7383\n",
      "Epoch 37/50\n",
      "3138/3138 [==============================] - 2s 482us/step - loss: 0.6522 - accuracy: 0.7395\n",
      "Epoch 38/50\n",
      "3138/3138 [==============================] - 2s 483us/step - loss: 0.6498 - accuracy: 0.7413\n",
      "Epoch 39/50\n",
      "3138/3138 [==============================] - 1s 472us/step - loss: 0.6445 - accuracy: 0.7447\n",
      "Epoch 40/50\n",
      "3138/3138 [==============================] - 1s 476us/step - loss: 0.6464 - accuracy: 0.7440\n",
      "Epoch 41/50\n",
      "3138/3138 [==============================] - 1s 445us/step - loss: 0.6421 - accuracy: 0.7447\n",
      "Epoch 42/50\n",
      "3138/3138 [==============================] - 1s 432us/step - loss: 0.6384 - accuracy: 0.7469\n",
      "Epoch 43/50\n",
      "3138/3138 [==============================] - 1s 448us/step - loss: 0.6369 - accuracy: 0.7468\n",
      "Epoch 44/50\n",
      "3138/3138 [==============================] - 1s 441us/step - loss: 0.6345 - accuracy: 0.7468\n",
      "Epoch 45/50\n",
      "3138/3138 [==============================] - 1s 448us/step - loss: 0.6343 - accuracy: 0.7489\n",
      "Epoch 46/50\n",
      "3138/3138 [==============================] - 1s 443us/step - loss: 0.6326 - accuracy: 0.74920s - loss:\n",
      "Epoch 47/50\n",
      "3138/3138 [==============================] - 1s 447us/step - loss: 0.6312 - accuracy: 0.7496\n",
      "Epoch 48/50\n",
      "3138/3138 [==============================] - 1s 444us/step - loss: 0.6285 - accuracy: 0.7510\n",
      "Epoch 49/50\n",
      "3138/3138 [==============================] - 1s 460us/step - loss: 0.6290 - accuracy: 0.7499\n",
      "Epoch 50/50\n",
      "3138/3138 [==============================] - 1s 449us/step - loss: 0.6260 - accuracy: 0.7512\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 886\n",
      "Trainable params: 886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network creation\n",
    "# input layer -> hidden layer (32 nodes) -> hidden layer (16 nodes) -> output layer (amount of classes)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.Input(shape=(7,)))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(all_classes.size)), # activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test) \n",
    "\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785/785 - 0s - loss: 0.6255 - accuracy: 0.7452\n",
      "\n",
      "Test accuracy: 0.745179295539856\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output layer needs to be converted to probabilities\n",
    "# probability_model = tf.keras.Sequential([model, \n",
    "#                                          tf.keras.layers.Softmax()])\n",
    "# predictions = probability_model.predict(X_test)\n",
    "\n",
    "# If softmax activation is used for output layer\n",
    "predictions = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1574707 ,  0.86816406, -0.41455078,  1.77001953, -4.8828125 ,\n",
       "        -2.86865234,  7.        ]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single prediction from manually chosen data\n",
    "singleAction = (np.expand_dims([-0.157470703125,0.8681640625,-0.41455078125,1.77001953125,-4.8828125,-2.86865234375, 7],0))\n",
    "singleAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['SideLunges']\n",
      "+------------+-------------+\n",
      "|   Action   | Probability |\n",
      "+------------+-------------+\n",
      "| BicepCurls |   433.5578  |\n",
      "|  Posture   |  -912.7257  |\n",
      "| SideLunges |  822.50574  |\n",
      "|  Sitting   |  -1467.5808 |\n",
      "|   Squats   |   820.1295  |\n",
      "|  Standing  |  -1656.9595 |\n",
      "+------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "singlePrediction = model.predict(singleAction)\n",
    "print(\"Predicted:\", le.inverse_transform(np.argmax(singlePrediction).ravel()))\n",
    "\n",
    "pretty_table = PrettyTable()\n",
    "pretty_table.add_column(\"Action\",all_classes)\n",
    "pretty_table.add_column(\"Probability\", singlePrediction[0]*100)\n",
    "print(pretty_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['Posture'] Actual: ['Posture']\n",
      "+------------+-------------+\n",
      "|   Action   | Probability |\n",
      "+------------+-------------+\n",
      "| BicepCurls |   660.5464  |\n",
      "|  Posture   |  1030.9507  |\n",
      "| SideLunges |  833.27216  |\n",
      "|  Sitting   |  -1133.3273 |\n",
      "|   Squats   |  728.61365  |\n",
      "|  Standing  |  -1620.3573 |\n",
      "+------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# All predictions from X_test\n",
    "index = 0\n",
    "print(\"Predicted:\", le.inverse_transform(np.argmax(predictions[index]).ravel()), \"Actual:\", le.inverse_transform(y_test[index].ravel()))\n",
    "\n",
    "pretty_table = PrettyTable()\n",
    "pretty_table.add_column(\"Action\",all_classes)\n",
    "pretty_table.add_column(\"Probability\", predictions[index]*100)\n",
    "print(pretty_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: actionModel/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the tensorflow model\n",
    "model.save('actionModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the tfmodel to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('actionModel') # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('converted_model_tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BicepCurls' 'Posture' 'SideLunges' 'Sitting' 'Squats' 'Standing']\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# # Path to data directory\n",
    "# path = r'/home/connor/Desktop/Pastebles/data'\n",
    "\n",
    "# # Holder list\n",
    "# li = []\n",
    "\n",
    "# # Regex changes the target class creation\n",
    "# # 'CORRECT|WRONG|[a-zA-Z]+(?=_[0-9]+_data)|(?<=IR_[0-9]_)[a-zA-Z]+'\n",
    "\n",
    "# # Will grab any subfolders from path and their csv files\n",
    "\n",
    "# for filename in Path(path).rglob('*.csv'):\n",
    "#     # Reads individual csv files\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0)\n",
    "#     # Matches specifics from the filename using regex (subject to change depending on filenaming convention)   \n",
    "#     match  = re.findall('[a-zA-Z]+(?=_[0-9]+_data)', str(filename))\n",
    "#     y = (''.join(match))\n",
    "#     # Adds target column for classification\n",
    "#     df['y'] = y\n",
    "#     # Appends the dataframe to the list\n",
    "#     li.append(df)\n",
    "\n",
    "# # Concats all data into one dataframe for training/testing\n",
    "# frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# # Target column is y\n",
    "# y_string = frame['y']\n",
    "# actions = ['BicepCurls', 'Posture', 'SideLunges', 'Sitting', 'Squats', 'Standing']\n",
    "\n",
    "# # Changes target from string to numeric\n",
    "# le = LabelEncoder().fit(actions)\n",
    "# y = le.transform(y_string.ravel())\n",
    "\n",
    "# print(le.classes_)\n",
    "# print(le.transform(le.classes_))\n",
    "\n",
    "# # Sets the X data\n",
    "# X = frame.drop(['y','arrival_time'],axis=1).to_numpy()\n",
    "\n",
    "# # Splits data into training and testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # random_state=42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
